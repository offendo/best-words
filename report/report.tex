%
% File acl2020.tex
%
%% Based on the style files for ACL 2020, which were
%% Based on the style files for ACL 2018, NAACL 2018/19, which were
%% Based on the style files for ACL-2015, with some improvements
%%  taken from the NAACL-2016 style
%% Based on the style files for ACL-2014, which were, in turn,
%% based on ACL-2013, ACL-2012, ACL-2011, ACL-2010, ACL-IJCNLP-2009,
%% EACL-2009, IJCNLP-2008...
%% Based on the style files for EACL 2006 by
%%e.agirre@ehu.es or Sergi.Balari@uab.es
%% and that of ACL 08 by Joakim Nivre and Noah Smith

\documentclass[11pt,a4paper]{article}
\usepackage[hyperref]{acl2020}
\usepackage{times}
\usepackage{latexsym}
\renewcommand{\UrlFont}{\ttfamily\small}

% This is not strictly necessary, and may be commented out,
% but it will improve the layout of the manuscript,
% and will typically save some space.
\usepackage{microtype}

\aclfinalcopy% Uncomment this line for the final submission
%\def\aclpaperid{***} %  Enter the acl Paper ID here

%\setlength\titlebox{5cm}
% You can expand the titlebox if you need extra space
% to show all the authors. Please do not make the titlebox
% smaller than 5cm (the original size); we will check this
% in the camera-ready version and ask you to change it back.

\newcommand\BibTeX{B\textsc{ib}\TeX}

\title{Do you \textit{Really} Have the Best Words}

\author{Kaleen Shrestha \\
  UC Santa Cruz\\
  \texttt{nilay@ucsc.edu} \\\And%
  Nilay Patel \\
  UC Santa Cruz\\
  \texttt{nilay@ucsc.edu} \\\And%
  Alex Lue \\
  UC Santa Cruz\\
  \texttt{alue@ucsc.edu} \\
}

\date{}

\begin{document}
\maketitle
\begin{abstract}
  This is the abstract. We can add it here if necessary.
\end{abstract}

\section{Sentence Selection}%
\label{sec:sentence-selection}

The second task in the pipeline is sentence selection, in which we choose a
subset of salient sentences from the retrieved articles. It is possible to
remove this step entirely, instead assuming all sentences have a non-zero
probability of saliency and letting the next item in the pipeline distinguish.
However, the baseline approach \cite{fever2018}, despite the imperfect recall,
improved overall performance when limiting the number of selected sentences.

\subsection{Previous Work}%
\label{subsec:sentence-selection-prev-work}

The baseline approach used bi-gram TF-IDF vectors for the claim and each target
vector, as well as their cosine similarity, and ranked the top-$k$ sentences. At
$k=5$ and $l=5$ documents, they achieve 55.3\% recall for verifiable claims.

\subsubsection{name}%
\label{subsubsec:label}



\subsection{Approach}%
\label{subsec:sentence-selection-approach}



\section{Natural Language Inference}%
\label{sec:nli}

\subsection{Previous Work}%
\label{subsec:nli-prev-work}

The winning team from the first FEVER task workshop UNC-NLP \citep{unc2018}
developed the \textbf{Neural Semantic Matching Network}, a four layer model
\footnotemark which performs semantic matching between two textual sequences.
Importantly, the team used no intermediate representation prior to training the
model, instead using a BiLSTM as an encoding layer.

\footnotetext{The four layers are the encoding layer, alignment layer, matching
layer, and output layer}

\subsection{Approach}%
\label{subsec:nli-approach}

We use


\bibliography{report}
\bibliographystyle{acl_natbib}

\end{document}
